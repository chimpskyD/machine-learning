{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beleg 1 - Implementierung der linearen Regression\n",
    "\n",
    "#### Vorgehen:\n",
    "\n",
    "1. Erstellen Sie zuerst zum Testen Ihrer Lösung automatische Daten: D.h. Punkte die auf einer Geraden liegen und deren y-Werte mittels eines gaussverteilten \"Rauschen\" von idealen Werten abweichen.\n",
    "2. Implementieren Sie die Hypothese - lineares Modell als Python Funktion. \n",
    "3. Implementieren Sie die Kostenfunktion J als Python Funktion.\n",
    "4. Plotten Sie die Kostenfunktion in der Umgebung des Minimums als Contourplot.\n",
    "5. Implementieren Sie das Gradientenabstiegsverfahren unter Benutzung der Kostenfunktion und der linearen Hypothese.\n",
    "6. Plotten Sie das Modell (Fit-Gerade) zusammen mit den Daten.\n",
    "7. Trainieren (siehe 5b) für verschiedene Werte der Lernrate und plotten Sie Kosten über den Iterationen in einen Graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Erstellung von Daten\n",
    "\n",
    "Normalverteilung mit numpy anhand der Methode normal:\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#Creation of array of x and y\n",
    "m = 30 #training examples\n",
    "low = 0\n",
    "high = 10\n",
    "\n",
    "x = np.linspace(low, high,num=m)\n",
    "y = np.linspace(low, high,num=m)\n",
    "\n",
    "#x= np.array([1.,2.,3])\n",
    "#y = np.array([.5, 2., 4.])\n",
    "\n",
    "mu = 0.0\n",
    "sigma = 1.5\n",
    "#Introduce error\n",
    "error = np.random.normal(mu, sigma, m)\n",
    "\n",
    "#y_error = y+error\n",
    "y = y+error\n",
    "print(y)\n",
    "#plt.hist(error, 50, edgecolor='black')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y, edgecolor='yellow', color='b')\n",
    "plt.plot(x, y-error, linewidth=4, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hypothese implementieren - Funktion von x\n",
    "Implementieren Sie die Hypothese (lineares Modell) als Python Funktion:\n",
    "- linear_hypothesis(theta_0, theta_1) \n",
    "Die Pythonfunktion soll dabei eine Funktion zurückgeben: \n",
    "- hypothesis = linear_hypothesis(2., 3.) \n",
    "- print hypothesis(np.array([1., 2.]))\n",
    "#[ 5.  8.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given thetas, calculate prediction for a given x value\n",
    "def linear_hypothesis(theta_0, theta_1):\n",
    "    return lambda x: theta_1*x+theta_0\n",
    "\n",
    "#Create linear function y = 3x+2\n",
    "hypothesis = linear_hypothesis(2., 3.)\n",
    "#Calculate linear function for x=1 and x=2\n",
    "print(hypothesis(np.array([1,2])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Kostenfunktion - Funktion der Parameter\n",
    "def cost_function(hypothesis, x, y)...\n",
    "\n",
    "Die Pythonfunktion soll dabei eine Funktion zurückgeben, die die beiden Parameter theta_0 und theta_1 aufnimmt.\n",
    "j = cost_function(linear_hypothesis, x, y) \n",
    "print j(2.1, 2.9)\n",
    "41.20  # Wert abhaengig von x und y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given an hypotesis, calculate cost function\n",
    "def cost_function(hypothesis,x,y, t0=0, t1=0):\n",
    "    m = float(len(x)) #training examples\n",
    "    #function for calculating \n",
    "    return lambda theta_0, theta_1: 1./(2*m)*((hypothesis(theta_0,theta_1)(x)-y)**2).sum()\n",
    "    #return 1./(2*m)*((linear_hypothesis(t0,t1)(x)-y)**2).sum()\n",
    "\n",
    "j = cost_function(linear_hypothesis, x, y)\n",
    "#print(j)\n",
    "print(j(2.9,2.7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plotten der Kostenfunktion\n",
    "\n",
    "Plotten Sie die Kostenfunktion in der Umgebung des Minimums als Contourplot.\n",
    "Verwenden Sie hierzu plt.contour(X,Y,Z) und zum Erzeugen des X-Y-Oberflaechengitters meshgrid(..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "a = 10\n",
    "b = 0\n",
    "ran = 30\n",
    "t0 = np.arange(a - ran, a + ran, ran * 0.05)\n",
    "t1 = np.arange(b - ran, b + ran, ran * 0.05)\n",
    "\n",
    "C = np.zeros([len(t0),len(t1)])\n",
    "c = cost_function(linear_hypothesis, x, y)\n",
    "\n",
    "for i, t_0 in enumerate(t0):\n",
    "    for j, t_1 in enumerate(t1):\n",
    "        C[j][i] = c(t_0, t_1)\n",
    "\n",
    "T0, T1 = np.meshgrid(t0, t1)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.contour(T0, T1, C)\n",
    "plt.xlabel('$\\Theta_0$')\n",
    "plt.ylabel('$\\Theta_1$')\n",
    "plt.title('Kostenfunktion')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradientenabstiegsverfahren\n",
    "\n",
    "Implementieren Sie das Gradientenabstiegsverfahren unter Benutzung der Kostenfunktion und der linearen Hypothese.\n",
    "\n",
    "Schreiben Sie eine Funktion die die Update Rules anwendet zur Berechnung der neuen theta-Werte:\n",
    "theta_0, theta_1 = compute_new_theta(x, y, theta_0, theta_1, alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates gradient for theta_0\n",
    "def gradient_theta1(theta_0, theta_1, x, y):\n",
    "    return lambda t1: 1./m *((linear_hypothesis(theta_0,theta_1)(x)-y)*x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates gradient for theta_1\n",
    "def gradient_theta0(theta_0, theta_1, x, y):\n",
    "    return lambda t0: 1./m *(linear_hypothesis(theta_0,theta_1)(x)-y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_new_theta(x,y,theta_0, theta_1, alpha):\n",
    "    #update theta_0\n",
    "    #t0 = theta_0 - alpha*gradient_theta0(theta_0, theta_1, x,y)(theta_0)\n",
    "    t0 = theta_0 - alpha*(1./m *(linear_hypothesis(theta_0,theta_1)(x)-y).sum())\n",
    "    #update theta_1\n",
    "    #t1 = theta_1 - alpha*gradient_theta1(theta_0, theta_1, x,y)(theta_1)\n",
    "    t1 = theta_1 - alpha*(1./m *((linear_hypothesis(theta_0,theta_1)(x)-y)*x).sum())\n",
    "    theta_0 = t0\n",
    "    theta_1 = t1\n",
    "    return theta_0, theta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b) Wählen Sie Startwerte in der Umgebung des Miniums der Kostenfunktion für theta. \n",
    "Wenden Sie iterativ die compute_new_theta Funktion an und finden Sie so ein Theta mit niedrigen Kosten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(x, y, theta_0=0, theta_1=0, alpha=0.0001):\n",
    "    costs = []\n",
    "    #t0 = theta_0\n",
    "    #t1 = theta_1\n",
    "    \n",
    "    iterations = 10000\n",
    "    counter = 0 \n",
    "    #start cost\n",
    "    j = cost_function(linear_hypothesis, x, y)(theta_0,theta_1)\n",
    "    #stores costs for each iteration\n",
    "    costs.append(j)\n",
    "    \n",
    "    #convergence value\n",
    "    convergence = 0.000001 \n",
    "    #costs are in each iteration updated\n",
    "    #When previous cost - current cost < convergence it should stop\n",
    "    #if not --> max 1000 iteration\n",
    "    cprev = j + 1  \n",
    "    theta0s = [theta_0]\n",
    "    theta1s = [theta_1]\n",
    "    \n",
    "    # When the costs converge or we hit a large number of iterations will we stop updating\n",
    "    while (np.abs(cprev - j) > convergence) and (counter < iterations):\n",
    "        cprev = j\n",
    "        theta_0, theta_1 = compute_new_theta(x,y,theta_0, theta_1, alpha)\n",
    "        \n",
    "        # Store thetas\n",
    "        theta0s.append(theta_0)\n",
    "        theta1s.append(theta_1)\n",
    "        \n",
    "        # Compute the new cost\n",
    "        j = cost_function(linear_hypothesis, x, y)(theta_1,theta_0)\n",
    "\n",
    "        # Store updates\n",
    "        costs.append(j)\n",
    "        counter += 1   # Count\n",
    "\n",
    "    return {'theta0': theta_0, 'theta1': theta_1, \"costs\": costs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gr = gradient_descent(x,y)\n",
    "#print(gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c) Plotten Sie den Fortschritt (Verringerung der Kosten über den Iterationen) für 5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(gr['costs'])), gr['costs'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modell plotten\n",
    "\n",
    "Plotten Sie das Modell (Fit-Gerade) zusammen mit den Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_x = gr['theta1']*x\n",
    "fit_y = fit_x+gr['theta0']\n",
    "\n",
    "plt.plot(fit_x, fit_y)\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Trainieren und plotten\n",
    "Trainieren (siehe 5b) für verschiedene Werte der Lernrate und plotten Sie Kosten über den Iterationen in einen Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr1 = gradient_descent(x,y,alpha=0.001)\n",
    "plt.plot(range(len(gr1['costs'])), gr1['costs'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr1 = gradient_descent(x,y,alpha=0.0001)\n",
    "plt.plot(range(len(gr1['costs'])), gr1['costs'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr1 = gradient_descent(x,y,alpha=0.00001)\n",
    "plt.plot(range(len(gr1['costs'])), gr1['costs'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr1 = gradient_descent(x,y,alpha=0.0000001)\n",
    "plt.plot(range(len(gr1['costs'])), gr1['costs'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr1 = gradient_descent(x,y,alpha=0.01)\n",
    "plt.plot(range(len(gr1['costs'])), gr1['costs'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
